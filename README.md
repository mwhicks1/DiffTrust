# DiffTrust: Estimating Correctness Without Oracles in LLM-Based Code Generation

This repository contains the implementation used in the experiments for the DiffTrust paper, which introduces *incoherence* as a theoretically grounded proxy for correctness in LLM-based code generationâ€”designed to operate without access to ground-truth implementations or oracles.

## Fork notes

This repository is a fork of the original, whose README continues below. Changes were made so that LLMs can be executed via AWS Bedrock. Note that you must use Python 3.12 to run the experiments. The script `analyze_experiment.py` will generate an incoherence vs. error plot after running an experiment.

## Overview

Large Language Models (LLMs) have demonstrated strong performance in code generation tasks, yet concerns about *confabulation* remainâ€”models frequently produce syntactically valid but semantically incorrect programs. In DiffTrust, we propose a principled proxy for correctness called **incoherence**, which quantifies semantic disagreement between independently sampled model outputs.

This repository supports empirical evaluation of our new metrics across two popular benchmarks: **HumanEval** and **MBPP**.

> **Note**: This repository is not intended as a general-purpose benchmarking toolkit, but rather as the exact implementation behind our experimental results.

## Paper Contributions (Recap)

1. **Incoherence** is proposed as an unsupervised, theoretically grounded estimator for correctness.
2. A **probabilistic framework** links incoherence to model error via a provable lower bound.
3. Empirical validation across 16 LLMs showing that incoherence alone can detect ~2/3 of incorrect progrms *without any false positives*, matching oracle-based rankings with Spearmanâ€™s Ï â‰¥ 0.92.

## Project Structure

```
.
â”œâ”€â”€ HumanEval
â”‚   â”œâ”€â”€ instance.py
â”‚   â”œâ”€â”€ remove_duplicates.py
â”‚   â”œâ”€â”€ run.py
â”‚   â””â”€â”€ stats.py
â”œâ”€â”€ MBPP
â”‚   â”œâ”€â”€ instance.py
â”‚   â”œâ”€â”€ remove_duplicates.py
â”‚   â”œâ”€â”€ run.py
â”‚   â””â”€â”€ stats.py
â”œâ”€â”€ README.md
â”œâ”€â”€ difftrust
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ core
â”‚   â”‚   â”œâ”€â”€ checking.py
â”‚   â”‚   â”œâ”€â”€ coder.py
â”‚   â”‚   â”œâ”€â”€ experiment.py
â”‚   â”‚   â”œâ”€â”€ function.py
â”‚   â”‚   â”œâ”€â”€ metrics.py
â”‚   â”‚   â”œâ”€â”€ refiner.py
â”‚   â”‚   â””â”€â”€ specification.py
â”‚   â”œâ”€â”€ fuzzer
â”‚   â”‚   â”œâ”€â”€ coverage.py
â”‚   â”‚   â””â”€â”€ fuzzer.py
â”‚   â”œâ”€â”€ generic
â”‚   â”‚   â”œâ”€â”€ generic_equal.py
â”‚   â”‚   â”œâ”€â”€ generic_explorer.py
â”‚   â”‚   â”œâ”€â”€ generic_fuzzer.py
â”‚   â”‚   â”œâ”€â”€ generic_mutator.py
â”‚   â”‚   â””â”€â”€ generic_repr.py
â”‚   â”œâ”€â”€ llm
â”‚   â”‚   â”œâ”€â”€ abstract.py
â”‚   â”‚   â”œâ”€â”€ chat.py
â”‚   â”‚   â”œâ”€â”€ chatgpt.py
â”‚   â”‚   â”œâ”€â”€ claude.py
â”‚   â”‚   â”œâ”€â”€ gemini.py
â”‚   â”‚   â”œâ”€â”€ open_router.py
â”‚   â”‚   â””â”€â”€ open_router_models.json
â”‚   â”œâ”€â”€ rqs
â”‚   â”‚   â”œâ”€â”€ ablation.py
â”‚   â”‚   â”œâ”€â”€ aggregate-plots
â”‚   â”‚   â”œâ”€â”€ constants.py
â”‚   â”‚   â”œâ”€â”€ costs_calculation.py
â”‚   â”‚   â”œâ”€â”€ counter.py
â”‚   â”‚   â”œâ”€â”€ custom_classes.py
â”‚   â”‚   â”œâ”€â”€ latex.py
â”‚   â”‚   â”œâ”€â”€ plots.py
â”‚   â”‚   â”œâ”€â”€ rq_utils.py
â”‚   â”‚   â””â”€â”€ utils.py
â”‚   â””â”€â”€ tracing
â”‚       â”œâ”€â”€ events.py
â”‚       â””â”€â”€ tracer.py
â””â”€â”€ requirements.txt

```

## Requirements

Install dependencies via:

```bash
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

## Running Experiments

Each dataset folder (e.g., `MBPP/` or `HumanEval/`) contains a `run.py` script that replicates the experimental setup from the paper.

### Example

```bash
# Run (from the root directory) the pointwise incoherence experiment on HumanEval
python -m HumanEval.run
```

To adjust parameters such as the LLM, number of candidate functions (`nb_candidate`), number of test inputs (`nb_sample`), or temperature settings.

### Key Parameters (set in `run.py`)
- `llm_name`: Name of the LLM to use (e.g., "gpt_4", "claude_opus_4", etc.)
- `nb_candidate`: Number of candidate programs to sample per task (default: 10)
- `nb_sample`: Number of test inputs per comparison (default: 1000)
- `temperature`: Sampling temperature for the LLM (default: 0.0 for deterministic outputs)
- `timeout`: Max execution time per comparison (default: 60 seconds)


[//]: # (## Citation)

[//]: # ()
[//]: # (> ðŸ“„ *Citation will be added once the paper is published.*)

## License

MIT License. See [LICENSE](./LICENSE) for ful text.
