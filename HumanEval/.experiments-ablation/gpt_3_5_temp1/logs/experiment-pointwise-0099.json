[
  {
    "name": "has_close_elements",
    "task_id": "HumanEval/0",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.730614185333252
  },
  {
    "name": "separate_paren_groups",
    "task_id": "HumanEval/1",
    "Dis": 0.541,
    "Err": 0.324,
    "TotalTime": 4.1973137855529785
  },
  {
    "name": "truncate_number",
    "task_id": "HumanEval/2",
    "Dis": 0.033,
    "Err": 0.204,
    "TotalTime": 2.127124547958374
  },
  {
    "name": "below_zero",
    "task_id": "HumanEval/3",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.697895050048828
  },
  {
    "name": "mean_absolute_deviation",
    "task_id": "HumanEval/4",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.4693734645843506
  },
  {
    "name": "intersperse",
    "task_id": "HumanEval/5",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.745811700820923
  },
  {
    "name": "parse_nested_parens",
    "task_id": "HumanEval/6",
    "Dis": 0.452,
    "Err": 0.483,
    "TotalTime": 3.7911198139190674
  },
  {
    "name": "filter_by_substring",
    "task_id": "HumanEval/7",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.5853846073150635
  },
  {
    "name": "sum_product",
    "task_id": "HumanEval/8",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.3796184062957764
  },
  {
    "name": "rolling_max",
    "task_id": "HumanEval/9",
    "Dis": 0.012,
    "Err": 0.03,
    "TotalTime": 3.1149280071258545
  },
  {
    "name": "make_palindrome",
    "task_id": "HumanEval/10",
    "Dis": 0.665,
    "Err": 0.684,
    "TotalTime": 3.405874252319336
  },
  {
    "name": "string_xor",
    "task_id": "HumanEval/11",
    "Dis": 0.574,
    "Err": 0.568,
    "TotalTime": 3.2626638412475586
  },
  {
    "name": "longest",
    "task_id": "HumanEval/12",
    "Dis": 0.075,
    "Err": 0.041,
    "TotalTime": 3.6128463745117188
  },
  {
    "name": "greatest_common_divisor",
    "task_id": "HumanEval/13",
    "Dis": 0.071,
    "Err": 0.036,
    "TotalTime": 3.389979839324951
  },
  {
    "name": "all_prefixes",
    "task_id": "HumanEval/14",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.7448840141296387
  },
  {
    "name": "string_sequence",
    "task_id": "HumanEval/15",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.3298988342285156
  },
  {
    "name": "count_distinct_characters",
    "task_id": "HumanEval/16",
    "Dis": 0.135,
    "Err": 0.084,
    "TotalTime": 2.9867734909057617
  },
  {
    "name": "parse_music",
    "task_id": "HumanEval/17",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.2520763874053955
  },
  {
    "name": "how_many_times",
    "task_id": "HumanEval/18",
    "Dis": 0.0,
    "Err": 0.06,
    "TotalTime": 2.824716567993164
  },
  {
    "name": "sort_numbers",
    "task_id": "HumanEval/19",
    "Dis": 0.003,
    "Err": 0.005,
    "TotalTime": 5.920454025268555
  },
  {
    "name": "find_closest_elements",
    "task_id": "HumanEval/20",
    "Dis": 0.002,
    "Err": 0.001,
    "TotalTime": 6.062023878097534
  },
  {
    "name": "rescale_to_unit",
    "task_id": "HumanEval/21",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.974654197692871
  },
  {
    "name": "filter_integers",
    "task_id": "HumanEval/22",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.409442663192749
  },
  {
    "name": "strlen",
    "task_id": "HumanEval/23",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.33661150932312
  },
  {
    "name": "largest_divisor",
    "task_id": "HumanEval/24",
    "Dis": 0.13,
    "Err": 0.115,
    "TotalTime": 2.695578098297119
  },
  {
    "name": "remove_duplicates",
    "task_id": "HumanEval/26",
    "Dis": 0.088,
    "Err": 0.407,
    "TotalTime": 2.875645399093628
  },
  {
    "name": "flip_case",
    "task_id": "HumanEval/27",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.856386661529541
  },
  {
    "name": "concatenate",
    "task_id": "HumanEval/28",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.628526449203491
  },
  {
    "name": "filter_by_prefix",
    "task_id": "HumanEval/29",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.8036372661590576
  },
  {
    "name": "get_positive",
    "task_id": "HumanEval/30",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.3658993244171143
  },
  {
    "name": "is_prime",
    "task_id": "HumanEval/31",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.17693305015564
  },
  {
    "name": "unique",
    "task_id": "HumanEval/34",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.4599480628967285
  },
  {
    "name": "max_element",
    "task_id": "HumanEval/35",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.0341899394989014
  }
]