[
  {
    "name": "has_close_elements",
    "task_id": "HumanEval/0",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.53658127784729
  },
  {
    "name": "separate_paren_groups",
    "task_id": "HumanEval/1",
    "Dis": 0.5191764705882352,
    "Err": 0.5287058823529411,
    "TotalTime": 3.154554843902588
  },
  {
    "name": "truncate_number",
    "task_id": "HumanEval/2",
    "Dis": 0.0,
    "Err": 0.21694117647058825,
    "TotalTime": 2.4309773445129395
  },
  {
    "name": "below_zero",
    "task_id": "HumanEval/3",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.796963691711426
  },
  {
    "name": "mean_absolute_deviation",
    "task_id": "HumanEval/4",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.032728910446167
  },
  {
    "name": "intersperse",
    "task_id": "HumanEval/5",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.6660284996032715
  },
  {
    "name": "parse_nested_parens",
    "task_id": "HumanEval/6",
    "Dis": 0.0,
    "Err": 0.3796470588235294,
    "TotalTime": 3.1082613468170166
  },
  {
    "name": "filter_by_substring",
    "task_id": "HumanEval/7",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 5.673011779785156
  },
  {
    "name": "sum_product",
    "task_id": "HumanEval/8",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.008569002151489
  },
  {
    "name": "rolling_max",
    "task_id": "HumanEval/9",
    "Dis": 0.0,
    "Err": 0.02176470588235294,
    "TotalTime": 3.8149254322052
  },
  {
    "name": "make_palindrome",
    "task_id": "HumanEval/10",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.0783164501190186
  },
  {
    "name": "string_xor",
    "task_id": "HumanEval/11",
    "Dis": 0.0,
    "Err": 0.9111764705882353,
    "TotalTime": 3.8446850776672363
  },
  {
    "name": "longest",
    "task_id": "HumanEval/12",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.405560493469238
  },
  {
    "name": "greatest_common_divisor",
    "task_id": "HumanEval/13",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.6030423641204834
  },
  {
    "name": "all_prefixes",
    "task_id": "HumanEval/14",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.1681413650512695
  },
  {
    "name": "string_sequence",
    "task_id": "HumanEval/15",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.3829782009124756
  },
  {
    "name": "count_distinct_characters",
    "task_id": "HumanEval/16",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.9317121505737305
  },
  {
    "name": "parse_music",
    "task_id": "HumanEval/17",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 6.744073867797852
  },
  {
    "name": "how_many_times",
    "task_id": "HumanEval/18",
    "Dis": 0.04305882352941177,
    "Err": 0.04788235294117647,
    "TotalTime": 3.7594053745269775
  },
  {
    "name": "sort_numbers",
    "task_id": "HumanEval/19",
    "Dis": 0.3591764705882353,
    "Err": 0.29294117647058826,
    "TotalTime": 9.367946147918701
  },
  {
    "name": "find_closest_elements",
    "task_id": "HumanEval/20",
    "Dis": 0.001176470588235294,
    "Err": 0.0016470588235294118,
    "TotalTime": 4.268341779708862
  },
  {
    "name": "rescale_to_unit",
    "task_id": "HumanEval/21",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.130928039550781
  },
  {
    "name": "filter_integers",
    "task_id": "HumanEval/22",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.739187002182007
  },
  {
    "name": "strlen",
    "task_id": "HumanEval/23",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.9147584438323975
  },
  {
    "name": "largest_divisor",
    "task_id": "HumanEval/24",
    "Dis": 0.04305882352941177,
    "Err": 0.2008235294117647,
    "TotalTime": 2.4543938636779785
  },
  {
    "name": "factorize",
    "task_id": "HumanEval/25",
    "Failed": "Timeout of 60.0 s. has been hit during disagreement computation"
  },
  {
    "name": "remove_duplicates",
    "task_id": "HumanEval/26",
    "Dis": 0.09094117647058823,
    "Err": 0.42176470588235293,
    "TotalTime": 3.86685848236084
  },
  {
    "name": "flip_case",
    "task_id": "HumanEval/27",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.908932685852051
  },
  {
    "name": "concatenate",
    "task_id": "HumanEval/28",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.779604434967041
  },
  {
    "name": "filter_by_prefix",
    "task_id": "HumanEval/29",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 6.755105257034302
  },
  {
    "name": "get_positive",
    "task_id": "HumanEval/30",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.083398342132568
  },
  {
    "name": "is_prime",
    "task_id": "HumanEval/31",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.6449050903320312
  },
  {
    "name": "unique",
    "task_id": "HumanEval/34",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.17864727973938
  },
  {
    "name": "max_element",
    "task_id": "HumanEval/35",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.060020446777344
  },
  {
    "name": "sort_even",
    "task_id": "HumanEval/37",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.218015670776367
  },
  {
    "name": "triples_sum_to_zero",
    "task_id": "HumanEval/40",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.053466081619263
  },
  {
    "name": "car_race_collision",
    "task_id": "HumanEval/41",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.3048787117004395
  },
  {
    "name": "incr_list",
    "task_id": "HumanEval/42",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.27758264541626
  },
  {
    "name": "pairs_sum_to_zero",
    "task_id": "HumanEval/43",
    "Dis": 0.0,
    "Err": 0.009764705882352941,
    "TotalTime": 3.925417900085449
  },
  {
    "name": "triangle_area",
    "task_id": "HumanEval/45",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.635533094406128
  },
  {
    "name": "fib4",
    "task_id": "HumanEval/46",
    "Dis": 0.01788235294117647,
    "Err": 0.01588235294117647,
    "TotalTime": 2.6250481605529785
  },
  {
    "name": "median",
    "task_id": "HumanEval/47",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.035757541656494
  },
  {
    "name": "is_palindrome",
    "task_id": "HumanEval/48",
    "Dis": 0.04411764705882353,
    "Err": 0.1028235294117647,
    "TotalTime": 2.9279329776763916
  },
  {
    "name": "modp",
    "task_id": "HumanEval/49",
    "Dis": 0.21458823529411764,
    "Err": 0.10541176470588236,
    "TotalTime": 2.881676435470581
  },
  {
    "name": "remove_vowels",
    "task_id": "HumanEval/51",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.0010058879852295
  },
  {
    "name": "below_threshold",
    "task_id": "HumanEval/52",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.07179594039917
  },
  {
    "name": "add",
    "task_id": "HumanEval/53",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.5428531169891357
  },
  {
    "name": "same_chars",
    "task_id": "HumanEval/54",
    "Dis": 0.001176470588235294,
    "Err": 0.004,
    "TotalTime": 3.968961000442505
  },
  {
    "name": "correct_bracketing",
    "task_id": "HumanEval/56",
    "Dis": 0.0,
    "Err": 0.40447058823529414,
    "TotalTime": 2.890376329421997
  },
  {
    "name": "monotonic",
    "task_id": "HumanEval/57",
    "Dis": 0.04611764705882353,
    "Err": 0.024,
    "TotalTime": 3.8534324169158936
  },
  {
    "name": "common",
    "task_id": "HumanEval/58",
    "Dis": 0.0,
    "Err": 0.3652941176470588,
    "TotalTime": 4.805525541305542
  },
  {
    "name": "largest_prime_factor",
    "task_id": "HumanEval/59",
    "Failed": "Timeout of 60.0 s. has been hit during disagreement computation"
  },
  {
    "name": "sum_to_n",
    "task_id": "HumanEval/60",
    "Dis": 0.04294117647058823,
    "Err": 0.02411764705882353,
    "TotalTime": 2.3859565258026123
  },
  {
    "name": "correct_bracketing",
    "task_id": "HumanEval/61",
    "Dis": 0.0,
    "Err": 0.41752941176470587,
    "TotalTime": 2.9533066749572754
  },
  {
    "name": "derivative",
    "task_id": "HumanEval/62",
    "Dis": 0.182,
    "Err": 0.088,
    "TotalTime": 3.7532289028167725
  },
  {
    "name": "vowels_count",
    "task_id": "HumanEval/64",
    "Dis": 0.0328235294117647,
    "Err": 0.017411764705882352,
    "TotalTime": 3.024000406265259
  },
  {
    "name": "circular_shift",
    "task_id": "HumanEval/65",
    "Dis": 0.23447058823529413,
    "Err": 0.23105882352941176,
    "TotalTime": 2.6209702491760254
  },
  {
    "name": "digitSum",
    "task_id": "HumanEval/66",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.9948723316192627
  },
  {
    "name": "fruit_distribution",
    "task_id": "HumanEval/67",
    "Dis": 0.5212941176470588,
    "Err": 0.7469411764705882,
    "TotalTime": 3.565056324005127
  },
  {
    "name": "pluck",
    "task_id": "HumanEval/68",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.03677773475647
  },
  {
    "name": "search",
    "task_id": "HumanEval/69",
    "Dis": 0.16270588235294117,
    "Err": 0.23023529411764707,
    "TotalTime": 5.026390075683594
  },
  {
    "name": "strange_sort_list",
    "task_id": "HumanEval/70",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.793367624282837
  },
  {
    "name": "triangle_area",
    "task_id": "HumanEval/71",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.914189100265503
  },
  {
    "name": "will_it_fly",
    "task_id": "HumanEval/72",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.395079851150513
  },
  {
    "name": "smallest_change",
    "task_id": "HumanEval/73",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.4451446533203125
  },
  {
    "name": "total_match",
    "task_id": "HumanEval/74",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 8.542810678482056
  },
  {
    "name": "iscube",
    "task_id": "HumanEval/77",
    "Dis": 0.5351764705882353,
    "Err": 0.5942352941176471,
    "TotalTime": 2.214384078979492
  },
  {
    "name": "hex_key",
    "task_id": "HumanEval/78",
    "Dis": 0.0,
    "Err": 0.03811764705882353,
    "TotalTime": 3.0384347438812256
  },
  {
    "name": "decimal_to_binary",
    "task_id": "HumanEval/79",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.358839511871338
  },
  {
    "name": "is_happy",
    "task_id": "HumanEval/80",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.9721896648406982
  },
  {
    "name": "numerical_letter_grade",
    "task_id": "HumanEval/81",
    "Dis": 0.4152941176470588,
    "Err": 0.7152941176470589,
    "TotalTime": 4.206533432006836
  },
  {
    "name": "prime_length",
    "task_id": "HumanEval/82",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.348737955093384
  },
  {
    "name": "starts_one_ends",
    "task_id": "HumanEval/83",
    "Failed": "Timeout of 60.0 s. has been hit during disagreement computation"
  },
  {
    "name": "solve",
    "task_id": "HumanEval/84",
    "Dis": 0.4596470588235294,
    "Err": 0.29223529411764704,
    "TotalTime": 2.5735082626342773
  },
  {
    "name": "add",
    "task_id": "HumanEval/85",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.832031011581421
  },
  {
    "name": "anti_shuffle",
    "task_id": "HumanEval/86",
    "Dis": 0.16470588235294117,
    "Err": 0.14458823529411766,
    "TotalTime": 3.0718283653259277
  },
  {
    "name": "get_row",
    "task_id": "HumanEval/87",
    "Dis": 0.10258823529411765,
    "Err": 0.06729411764705882,
    "TotalTime": 7.671305418014526
  },
  {
    "name": "sort_array",
    "task_id": "HumanEval/88",
    "Dis": 0.01,
    "Err": 0.008588235294117647,
    "TotalTime": 4.27314305305481
  },
  {
    "name": "encrypt",
    "task_id": "HumanEval/89",
    "Dis": 0.25176470588235295,
    "Err": 0.45176470588235296,
    "TotalTime": 3.2750861644744873
  },
  {
    "name": "next_smallest",
    "task_id": "HumanEval/90",
    "Dis": 0.0,
    "Err": 0.009294117647058824,
    "TotalTime": 4.084291458129883
  },
  {
    "name": "is_bored",
    "task_id": "HumanEval/91",
    "Dis": 0.7094117647058824,
    "Err": 0.8252941176470588,
    "TotalTime": 3.1368918418884277
  },
  {
    "name": "any_int",
    "task_id": "HumanEval/92",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.1584253311157227
  },
  {
    "name": "encode",
    "task_id": "HumanEval/93",
    "Dis": 0.15611764705882353,
    "Err": 0.4505882352941176,
    "TotalTime": 3.199371099472046
  },
  {
    "name": "skjkasdkd",
    "task_id": "HumanEval/94",
    "Dis": 0.0,
    "Err": 0.01411764705882353,
    "TotalTime": 5.572349786758423
  },
  {
    "name": "check_dict_case",
    "task_id": "HumanEval/95",
    "Dis": 0.3664705882352941,
    "Err": 0.8014117647058824,
    "TotalTime": 4.066083669662476
  },
  {
    "name": "count_up_to",
    "task_id": "HumanEval/96",
    "Dis": 0.3307058823529412,
    "Err": 0.21435294117647058,
    "TotalTime": 3.418390989303589
  },
  {
    "name": "multiply",
    "task_id": "HumanEval/97",
    "Dis": 0.0,
    "Err": 0.22258823529411764,
    "TotalTime": 2.6746513843536377
  },
  {
    "name": "count_upper",
    "task_id": "HumanEval/98",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.949063777923584
  },
  {
    "name": "closest_integer",
    "task_id": "HumanEval/99",
    "Dis": 0.06623529411764706,
    "Err": 0.07588235294117647,
    "TotalTime": 6.775584697723389
  },
  {
    "name": "make_a_pile",
    "task_id": "HumanEval/100",
    "Dis": 0.2596470588235294,
    "Err": 0.6724705882352942,
    "TotalTime": 2.5274405479431152
  },
  {
    "name": "words_string",
    "task_id": "HumanEval/101",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.0521962642669678
  },
  {
    "name": "choose_num",
    "task_id": "HumanEval/102",
    "Dis": 0.31011764705882355,
    "Err": 0.3763529411764706,
    "TotalTime": 2.5995044708251953
  },
  {
    "name": "rounded_avg",
    "task_id": "HumanEval/103",
    "Dis": 0.06247058823529412,
    "Err": 0.0671764705882353,
    "TotalTime": 2.797449827194214
  },
  {
    "name": "unique_digits",
    "task_id": "HumanEval/104",
    "Dis": 0.018941176470588236,
    "Err": 0.010705882352941176,
    "TotalTime": 4.30934739112854
  },
  {
    "name": "by_length",
    "task_id": "HumanEval/105",
    "Dis": 0.007176470588235294,
    "Err": 0.06541176470588235,
    "TotalTime": 3.9785618782043457
  },
  {
    "name": "f",
    "task_id": "HumanEval/106",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.2579755783081055
  },
  {
    "name": "even_odd_palindrome",
    "task_id": "HumanEval/107",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.942387580871582
  },
  {
    "name": "count_nums",
    "task_id": "HumanEval/108",
    "Dis": 0.5136470588235295,
    "Err": 0.41423529411764703,
    "TotalTime": 4.330444097518921
  },
  {
    "name": "move_one_ball",
    "task_id": "HumanEval/109",
    "Dis": 0.05423529411764706,
    "Err": 0.028,
    "TotalTime": 3.7693257331848145
  },
  {
    "name": "exchange",
    "task_id": "HumanEval/110",
    "Dis": 0.055411764705882355,
    "Err": 0.03988235294117647,
    "TotalTime": 7.002822160720825
  },
  {
    "name": "histogram",
    "task_id": "HumanEval/111",
    "Dis": 0.003176470588235294,
    "Err": 0.316,
    "TotalTime": 3.0515317916870117
  },
  {
    "name": "reverse_delete",
    "task_id": "HumanEval/112",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.7857742309570312
  },
  {
    "name": "odd_count",
    "task_id": "HumanEval/113",
    "Dis": 0.010588235294117647,
    "Err": 0.026705882352941177,
    "TotalTime": 7.127791166305542
  },
  {
    "name": "minSubArraySum",
    "task_id": "HumanEval/114",
    "Dis": 0.12094117647058823,
    "Err": 0.060941176470588235,
    "TotalTime": 3.954575300216675
  },
  {
    "name": "max_fill",
    "task_id": "HumanEval/115",
    "Dis": 0.7187058823529412,
    "Err": 0.7917647058823529,
    "TotalTime": 6.333693981170654
  },
  {
    "name": "sort_array",
    "task_id": "HumanEval/116",
    "Dis": 0.2777647058823529,
    "Err": 0.2043529411764706,
    "TotalTime": 4.1650567054748535
  },
  {
    "name": "select_words",
    "task_id": "HumanEval/117",
    "Dis": 0.0051764705882352945,
    "Err": 0.05423529411764706,
    "TotalTime": 3.4393577575683594
  },
  {
    "name": "get_closest_vowel",
    "task_id": "HumanEval/118",
    "Dis": 0.018705882352941176,
    "Err": 0.009294117647058824,
    "TotalTime": 2.9867613315582275
  },
  {
    "name": "match_parens",
    "task_id": "HumanEval/119",
    "Dis": 0.13823529411764707,
    "Err": 0.5456470588235294,
    "TotalTime": 5.0641069412231445
  },
  {
    "name": "maximum",
    "task_id": "HumanEval/120",
    "Dis": 0.0,
    "Err": 0.5551764705882353,
    "TotalTime": 4.255859613418579
  },
  {
    "name": "solution",
    "task_id": "HumanEval/121",
    "Dis": 0.0,
    "Err": 0.03058823529411765,
    "TotalTime": 3.642681360244751
  },
  {
    "name": "add_elements",
    "task_id": "HumanEval/122",
    "Dis": 0.20905882352941177,
    "Err": 0.4464705882352941,
    "TotalTime": 4.126027822494507
  },
  {
    "name": "get_odd_collatz",
    "task_id": "HumanEval/123",
    "Failed": "Timeout of 60.0 s. has been hit during disagreement computation"
  },
  {
    "name": "valid_date",
    "task_id": "HumanEval/124",
    "Dis": 0.008588235294117647,
    "Err": 0.020352941176470588,
    "TotalTime": 3.118457317352295
  },
  {
    "name": "split_words",
    "task_id": "HumanEval/125",
    "Dis": 0.18564705882352942,
    "Err": 0.12858823529411764,
    "TotalTime": 2.9995527267456055
  },
  {
    "name": "is_sorted",
    "task_id": "HumanEval/126",
    "Dis": 0.0008235294117647059,
    "Err": 0.09376470588235294,
    "TotalTime": 4.090048313140869
  },
  {
    "name": "intersection",
    "task_id": "HumanEval/127",
    "Dis": 0.26635294117647057,
    "Err": 0.24870588235294117,
    "TotalTime": 9.348098039627075
  },
  {
    "name": "prod_signs",
    "task_id": "HumanEval/128",
    "Dis": 0.14729411764705883,
    "Err": 0.09705882352941177,
    "TotalTime": 4.06151819229126
  },
  {
    "name": "minPath",
    "task_id": "HumanEval/129",
    "Failed": "Timeout of 60.0 s. has been hit during disagreement computation"
  },
  {
    "name": "tri",
    "task_id": "HumanEval/130",
    "Dis": 0.0,
    "Err": 1.0,
    "TotalTime": 2.3322298526763916
  },
  {
    "name": "digits",
    "task_id": "HumanEval/131",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.3433287143707275
  },
  {
    "name": "is_nested",
    "task_id": "HumanEval/132",
    "Dis": 0.41223529411764703,
    "Err": 0.4138823529411765,
    "TotalTime": 3.0157198905944824
  },
  {
    "name": "sum_squares",
    "task_id": "HumanEval/133",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.949032783508301
  },
  {
    "name": "check_if_last_char_is_a_letter",
    "task_id": "HumanEval/134",
    "Dis": 0.28247058823529414,
    "Err": 0.3088235294117647,
    "TotalTime": 2.9503049850463867
  },
  {
    "name": "can_arrange",
    "task_id": "HumanEval/135",
    "Dis": 0.3947058823529412,
    "Err": 0.24670588235294116,
    "TotalTime": 3.7984201908111572
  },
  {
    "name": "largest_smallest_integers",
    "task_id": "HumanEval/136",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.9104042053222656
  },
  {
    "name": "compare_one",
    "task_id": "HumanEval/137",
    "Dis": 0.4623529411764706,
    "Err": 0.2969411764705882,
    "TotalTime": 3.738598108291626
  },
  {
    "name": "is_equal_to_sum_even",
    "task_id": "HumanEval/138",
    "Dis": 0.10105882352941177,
    "Err": 0.12576470588235295,
    "TotalTime": 2.4376087188720703
  },
  {
    "name": "special_factorial",
    "task_id": "HumanEval/139",
    "Failed": "Timeout of 60.0 s. has been hit during disagreement computation"
  },
  {
    "name": "fix_spaces",
    "task_id": "HumanEval/140",
    "Dis": 0.5147058823529411,
    "Err": 0.4768235294117647,
    "TotalTime": 3.0001630783081055
  },
  {
    "name": "file_name_check",
    "task_id": "HumanEval/141",
    "Dis": 0.014,
    "Err": 0.009529411764705882,
    "TotalTime": 2.934722900390625
  },
  {
    "name": "sum_squares",
    "task_id": "HumanEval/142",
    "Dis": 0.0,
    "Err": 0.8070588235294117,
    "TotalTime": 4.450387001037598
  },
  {
    "name": "words_in_sentence",
    "task_id": "HumanEval/143",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.4455044269561768
  },
  {
    "name": "simplify",
    "task_id": "HumanEval/144",
    "Dis": 0.18741176470588236,
    "Err": 0.09447058823529411,
    "TotalTime": 109.58115124702454
  },
  {
    "name": "order_by_points",
    "task_id": "HumanEval/145",
    "Dis": 0.03423529411764706,
    "Err": 0.6869411764705883,
    "TotalTime": 4.759113788604736
  },
  {
    "name": "specialFilter",
    "task_id": "HumanEval/146",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.983450174331665
  },
  {
    "name": "bf",
    "task_id": "HumanEval/148",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.76587176322937
  },
  {
    "name": "sorted_list_sum",
    "task_id": "HumanEval/149",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 5.070625305175781
  },
  {
    "name": "x_or_y",
    "task_id": "HumanEval/150",
    "Dis": 0.0,
    "Err": 0.21964705882352942,
    "TotalTime": 3.438930034637451
  },
  {
    "name": "double_the_difference",
    "task_id": "HumanEval/151",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 6.083638668060303
  },
  {
    "name": "compare",
    "task_id": "HumanEval/152",
    "Dis": 0.4430588235294118,
    "Err": 0.5649411764705883,
    "TotalTime": 6.636779069900513
  },
  {
    "name": "Strongest_Extension",
    "task_id": "HumanEval/153",
    "Dis": 0.010588235294117647,
    "Err": 0.0055294117647058825,
    "TotalTime": 6.436291694641113
  },
  {
    "name": "cycpattern_check",
    "task_id": "HumanEval/154",
    "Dis": 0.0,
    "Err": 0.04188235294117647,
    "TotalTime": 3.9394657611846924
  },
  {
    "name": "even_odd_count",
    "task_id": "HumanEval/155",
    "Dis": 0.5527058823529412,
    "Err": 0.5014117647058823,
    "TotalTime": 2.26413893699646
  },
  {
    "name": "right_angle_triangle",
    "task_id": "HumanEval/157",
    "Dis": 0.0,
    "Err": 0.0010588235294117646,
    "TotalTime": 2.8556480407714844
  },
  {
    "name": "find_max",
    "task_id": "HumanEval/158",
    "Dis": 0.017764705882352943,
    "Err": 0.032,
    "TotalTime": 4.563464879989624
  },
  {
    "name": "eat",
    "task_id": "HumanEval/159",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.87642765045166
  },
  {
    "name": "solve",
    "task_id": "HumanEval/161",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.9463512897491455
  },
  {
    "name": "string_to_md5",
    "task_id": "HumanEval/162",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.411511182785034
  },
  {
    "name": "generate_integers",
    "task_id": "HumanEval/163",
    "Dis": 0.11235294117647059,
    "Err": 0.9762352941176471,
    "TotalTime": 2.8075597286224365
  }
]