[
  {
    "name": "has_close_elements",
    "task_id": "HumanEval/0",
    "Dis": 0.0,
    "Err": 0.058,
    "TotalTime": 3.4457075595855713
  },
  {
    "name": "separate_paren_groups",
    "task_id": "HumanEval/1",
    "Dis": 0.76,
    "Err": 0.6341666666666667,
    "TotalTime": 2.368687391281128
  },
  {
    "name": "truncate_number",
    "task_id": "HumanEval/2",
    "Dis": 0.07816666666666666,
    "Err": 0.16933333333333334,
    "TotalTime": 1.7924015522003174
  },
  {
    "name": "below_zero",
    "task_id": "HumanEval/3",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.9470698833465576
  },
  {
    "name": "mean_absolute_deviation",
    "task_id": "HumanEval/4",
    "Dis": 0.00016666666666666666,
    "Err": 0.0,
    "TotalTime": 3.2017080783843994
  },
  {
    "name": "intersperse",
    "task_id": "HumanEval/5",
    "Dis": 0.0425,
    "Err": 0.021,
    "TotalTime": 2.760709285736084
  },
  {
    "name": "parse_nested_parens",
    "task_id": "HumanEval/6",
    "Dis": 0.182,
    "Err": 0.421,
    "TotalTime": 2.3946938514709473
  },
  {
    "name": "filter_by_substring",
    "task_id": "HumanEval/7",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.434582471847534
  },
  {
    "name": "sum_product",
    "task_id": "HumanEval/8",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.0180740356445312
  },
  {
    "name": "rolling_max",
    "task_id": "HumanEval/9",
    "Dis": 0.04583333333333333,
    "Err": 0.085,
    "TotalTime": 2.9719443321228027
  },
  {
    "name": "make_palindrome",
    "task_id": "HumanEval/10",
    "Dis": 0.7366666666666667,
    "Err": 0.64,
    "TotalTime": 2.2538881301879883
  },
  {
    "name": "string_xor",
    "task_id": "HumanEval/11",
    "Dis": 0.7263333333333334,
    "Err": 0.7421666666666666,
    "TotalTime": 2.95259428024292
  },
  {
    "name": "longest",
    "task_id": "HumanEval/12",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.3146989345550537
  },
  {
    "name": "greatest_common_divisor",
    "task_id": "HumanEval/13",
    "Dis": 0.10566666666666667,
    "Err": 0.12116666666666667,
    "TotalTime": 1.932830810546875
  },
  {
    "name": "all_prefixes",
    "task_id": "HumanEval/14",
    "Dis": 0.1555,
    "Err": 0.072,
    "TotalTime": 2.3068008422851562
  },
  {
    "name": "string_sequence",
    "task_id": "HumanEval/15",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.8263418674468994
  },
  {
    "name": "count_distinct_characters",
    "task_id": "HumanEval/16",
    "Dis": 0.25333333333333335,
    "Err": 0.13766666666666666,
    "TotalTime": 2.221914291381836
  },
  {
    "name": "parse_music",
    "task_id": "HumanEval/17",
    "Dis": 0.316,
    "Err": 0.177,
    "TotalTime": 5.075876712799072
  },
  {
    "name": "how_many_times",
    "task_id": "HumanEval/18",
    "Dis": 0.1535,
    "Err": 0.11233333333333333,
    "TotalTime": 2.9975080490112305
  },
  {
    "name": "sort_numbers",
    "task_id": "HumanEval/19",
    "Dis": 0.1235,
    "Err": 0.0655,
    "TotalTime": 7.133890151977539
  },
  {
    "name": "find_closest_elements",
    "task_id": "HumanEval/20",
    "Dis": 0.033,
    "Err": 0.0305,
    "TotalTime": 3.3163435459136963
  },
  {
    "name": "rescale_to_unit",
    "task_id": "HumanEval/21",
    "Dis": 0.00016666666666666666,
    "Err": 0.0,
    "TotalTime": 3.3539042472839355
  },
  {
    "name": "filter_integers",
    "task_id": "HumanEval/22",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.8765347003936768
  },
  {
    "name": "strlen",
    "task_id": "HumanEval/23",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.2175309658050537
  },
  {
    "name": "largest_divisor",
    "task_id": "HumanEval/24",
    "Dis": 0.08033333333333334,
    "Err": 0.171,
    "TotalTime": 1.8069305419921875
  },
  {
    "name": "factorize",
    "task_id": "HumanEval/25",
    "Dis": 0.129,
    "Err": 0.0765,
    "TotalTime": 6.893471956253052
  },
  {
    "name": "remove_duplicates",
    "task_id": "HumanEval/26",
    "Dis": 0.0,
    "Err": 0.4726666666666667,
    "TotalTime": 2.9968364238739014
  },
  {
    "name": "flip_case",
    "task_id": "HumanEval/27",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.328463315963745
  },
  {
    "name": "concatenate",
    "task_id": "HumanEval/28",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.7650856971740723
  },
  {
    "name": "filter_by_prefix",
    "task_id": "HumanEval/29",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 5.21435546875
  },
  {
    "name": "get_positive",
    "task_id": "HumanEval/30",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.3167569637298584
  },
  {
    "name": "is_prime",
    "task_id": "HumanEval/31",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.0840694904327393
  },
  {
    "name": "unique",
    "task_id": "HumanEval/34",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.3337385654449463
  },
  {
    "name": "max_element",
    "task_id": "HumanEval/35",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.2655744552612305
  },
  {
    "name": "sort_even",
    "task_id": "HumanEval/37",
    "Dis": 0.35233333333333333,
    "Err": 0.19183333333333333,
    "TotalTime": 3.1040167808532715
  },
  {
    "name": "triples_sum_to_zero",
    "task_id": "HumanEval/40",
    "Dis": 0.11483333333333333,
    "Err": 0.06966666666666667,
    "TotalTime": 3.0286340713500977
  },
  {
    "name": "car_race_collision",
    "task_id": "HumanEval/41",
    "Dis": 0.7025,
    "Err": 0.9731666666666666,
    "TotalTime": 1.8183927536010742
  },
  {
    "name": "incr_list",
    "task_id": "HumanEval/42",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.258516311645508
  },
  {
    "name": "pairs_sum_to_zero",
    "task_id": "HumanEval/43",
    "Dis": 0.0,
    "Err": 0.0105,
    "TotalTime": 3.0787835121154785
  },
  {
    "name": "triangle_area",
    "task_id": "HumanEval/45",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.9389591217041016
  },
  {
    "name": "fib4",
    "task_id": "HumanEval/46",
    "Failed": "Exception occurred during compilation of LLM-generated code -- TimeoutError : "
  },
  {
    "name": "median",
    "task_id": "HumanEval/47",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.030212879180908
  },
  {
    "name": "is_palindrome",
    "task_id": "HumanEval/48",
    "Dis": 0.0023333333333333335,
    "Err": 0.0005,
    "TotalTime": 2.1868293285369873
  },
  {
    "name": "modp",
    "task_id": "HumanEval/49",
    "Dis": 0.0685,
    "Err": 0.22333333333333333,
    "TotalTime": 3.5463104248046875
  },
  {
    "name": "remove_vowels",
    "task_id": "HumanEval/51",
    "Dis": 0.22116666666666668,
    "Err": 0.11483333333333333,
    "TotalTime": 2.3024890422821045
  },
  {
    "name": "below_threshold",
    "task_id": "HumanEval/52",
    "Dis": 0.0055,
    "Err": 0.0021666666666666666,
    "TotalTime": 3.153214693069458
  },
  {
    "name": "add",
    "task_id": "HumanEval/53",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.9964683055877686
  },
  {
    "name": "same_chars",
    "task_id": "HumanEval/54",
    "Dis": 0.0026666666666666666,
    "Err": 0.004333333333333333,
    "TotalTime": 2.9697928428649902
  },
  {
    "name": "correct_bracketing",
    "task_id": "HumanEval/56",
    "Dis": 0.0,
    "Err": 0.4046666666666667,
    "TotalTime": 2.264554500579834
  },
  {
    "name": "monotonic",
    "task_id": "HumanEval/57",
    "Dis": 0.0205,
    "Err": 0.012166666666666666,
    "TotalTime": 2.994640350341797
  },
  {
    "name": "common",
    "task_id": "HumanEval/58",
    "Dis": 0.22433333333333333,
    "Err": 0.4915,
    "TotalTime": 3.86820650100708
  },
  {
    "name": "largest_prime_factor",
    "task_id": "HumanEval/59",
    "Dis": 0.09083333333333334,
    "Err": 0.0805,
    "TotalTime": 6.102268218994141
  },
  {
    "name": "sum_to_n",
    "task_id": "HumanEval/60",
    "Dis": 0.117,
    "Err": 0.173,
    "TotalTime": 1.760549545288086
  },
  {
    "name": "correct_bracketing",
    "task_id": "HumanEval/61",
    "Dis": 0.03266666666666666,
    "Err": 0.41933333333333334,
    "TotalTime": 2.225632905960083
  },
  {
    "name": "derivative",
    "task_id": "HumanEval/62",
    "Dis": 0.5315,
    "Err": 0.2955,
    "TotalTime": 2.7815239429473877
  },
  {
    "name": "vowels_count",
    "task_id": "HumanEval/64",
    "Dis": 0.044333333333333336,
    "Err": 0.023666666666666666,
    "TotalTime": 2.246974468231201
  },
  {
    "name": "circular_shift",
    "task_id": "HumanEval/65",
    "Dis": 0.30233333333333334,
    "Err": 0.3948333333333333,
    "TotalTime": 1.9684078693389893
  },
  {
    "name": "digitSum",
    "task_id": "HumanEval/66",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.2583584785461426
  },
  {
    "name": "fruit_distribution",
    "task_id": "HumanEval/67",
    "Dis": 0.55,
    "Err": 0.7861666666666667,
    "TotalTime": 2.6385414600372314
  },
  {
    "name": "pluck",
    "task_id": "HumanEval/68",
    "Dis": 0.18083333333333335,
    "Err": 0.09016666666666667,
    "TotalTime": 3.0735104084014893
  },
  {
    "name": "search",
    "task_id": "HumanEval/69",
    "Dis": 0.1115,
    "Err": 0.17383333333333334,
    "TotalTime": 3.7674291133880615
  },
  {
    "name": "strange_sort_list",
    "task_id": "HumanEval/70",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.8470571041107178
  },
  {
    "name": "triangle_area",
    "task_id": "HumanEval/71",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.2135488986968994
  },
  {
    "name": "will_it_fly",
    "task_id": "HumanEval/72",
    "Dis": 0.014166666666666666,
    "Err": 0.005666666666666667,
    "TotalTime": 3.335677146911621
  },
  {
    "name": "smallest_change",
    "task_id": "HumanEval/73",
    "Dis": 0.6328333333333334,
    "Err": 0.7671666666666667,
    "TotalTime": 2.5726685523986816
  },
  {
    "name": "total_match",
    "task_id": "HumanEval/74",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 6.148649215698242
  },
  {
    "name": "iscube",
    "task_id": "HumanEval/77",
    "Dis": 0.39116666666666666,
    "Err": 0.37916666666666665,
    "TotalTime": 1.6679718494415283
  },
  {
    "name": "hex_key",
    "task_id": "HumanEval/78",
    "Dis": 0.21666666666666667,
    "Err": 0.17266666666666666,
    "TotalTime": 2.2582621574401855
  },
  {
    "name": "decimal_to_binary",
    "task_id": "HumanEval/79",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.6709620952606201
  },
  {
    "name": "is_happy",
    "task_id": "HumanEval/80",
    "Dis": 0.1575,
    "Err": 0.08516666666666667,
    "TotalTime": 2.2285268306732178
  },
  {
    "name": "numerical_letter_grade",
    "task_id": "HumanEval/81",
    "Dis": 0.5386666666666666,
    "Err": 0.5011666666666666,
    "TotalTime": 3.0123448371887207
  },
  {
    "name": "prime_length",
    "task_id": "HumanEval/82",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.165191650390625
  },
  {
    "name": "starts_one_ends",
    "task_id": "HumanEval/83",
    "Dis": 0.8203333333333334,
    "Err": 0.9863333333333333,
    "TotalTime": 1.7121758460998535
  },
  {
    "name": "solve",
    "task_id": "HumanEval/84",
    "Dis": 0.5373333333333333,
    "Err": 0.896,
    "TotalTime": 1.7630822658538818
  },
  {
    "name": "add",
    "task_id": "HumanEval/85",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.5799708366394043
  },
  {
    "name": "anti_shuffle",
    "task_id": "HumanEval/86",
    "Dis": 0.32133333333333336,
    "Err": 0.38416666666666666,
    "TotalTime": 2.152998208999634
  },
  {
    "name": "get_row",
    "task_id": "HumanEval/87",
    "Dis": 0.13316666666666666,
    "Err": 0.097,
    "TotalTime": 4.850278854370117
  },
  {
    "name": "sort_array",
    "task_id": "HumanEval/88",
    "Dis": 0.16933333333333334,
    "Err": 0.07966666666666666,
    "TotalTime": 2.99753999710083
  },
  {
    "name": "encrypt",
    "task_id": "HumanEval/89",
    "Dis": 0.4671666666666667,
    "Err": 0.461,
    "TotalTime": 2.2623579502105713
  },
  {
    "name": "next_smallest",
    "task_id": "HumanEval/90",
    "Dis": 0.24616666666666667,
    "Err": 0.16866666666666666,
    "TotalTime": 2.8484747409820557
  },
  {
    "name": "is_bored",
    "task_id": "HumanEval/91",
    "Dis": 0.035666666666666666,
    "Err": 0.148,
    "TotalTime": 2.189793109893799
  },
  {
    "name": "any_int",
    "task_id": "HumanEval/92",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.1919326782226562
  },
  {
    "name": "encode",
    "task_id": "HumanEval/93",
    "Dis": 0.6253333333333333,
    "Err": 0.6695,
    "TotalTime": 2.2815370559692383
  },
  {
    "name": "skjkasdkd",
    "task_id": "HumanEval/94",
    "Dis": 0.060833333333333336,
    "Err": 0.0815,
    "TotalTime": 3.656682014465332
  },
  {
    "name": "check_dict_case",
    "task_id": "HumanEval/95",
    "Dis": 0.17066666666666666,
    "Err": 0.11983333333333333,
    "TotalTime": 2.9626357555389404
  },
  {
    "name": "count_up_to",
    "task_id": "HumanEval/96",
    "Dis": 0.05333333333333334,
    "Err": 0.023666666666666666,
    "TotalTime": 2.160985231399536
  },
  {
    "name": "multiply",
    "task_id": "HumanEval/97",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.904465913772583
  },
  {
    "name": "count_upper",
    "task_id": "HumanEval/98",
    "Dis": 0.11016666666666666,
    "Err": 0.08416666666666667,
    "TotalTime": 2.1516666412353516
  },
  {
    "name": "closest_integer",
    "task_id": "HumanEval/99",
    "Dis": 0.36816666666666664,
    "Err": 0.25833333333333336,
    "TotalTime": 4.729492902755737
  },
  {
    "name": "make_a_pile",
    "task_id": "HumanEval/100",
    "Failed": "Timeout of 60.0 s. has been hit during disagreement computation"
  },
  {
    "name": "words_string",
    "task_id": "HumanEval/101",
    "Dis": 0.04933333333333333,
    "Err": 0.0225,
    "TotalTime": 2.303025960922241
  },
  {
    "name": "choose_num",
    "task_id": "HumanEval/102",
    "Dis": 0.337,
    "Err": 0.44666666666666666,
    "TotalTime": 2.6087143421173096
  },
  {
    "name": "rounded_avg",
    "task_id": "HumanEval/103",
    "Dis": 0.46016666666666667,
    "Err": 0.4678333333333333,
    "TotalTime": 2.1071181297302246
  },
  {
    "name": "unique_digits",
    "task_id": "HumanEval/104",
    "Dis": 0.004166666666666667,
    "Err": 0.0003333333333333333,
    "TotalTime": 3.2814667224884033
  },
  {
    "name": "by_length",
    "task_id": "HumanEval/105",
    "Dis": 0.3631666666666667,
    "Err": 0.26966666666666667,
    "TotalTime": 2.9988059997558594
  },
  {
    "name": "f",
    "task_id": "HumanEval/106",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.1280677318573
  },
  {
    "name": "even_odd_palindrome",
    "task_id": "HumanEval/107",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.296046495437622
  },
  {
    "name": "count_nums",
    "task_id": "HumanEval/108",
    "Dis": 0.48133333333333334,
    "Err": 0.51,
    "TotalTime": 3.3228988647460938
  },
  {
    "name": "move_one_ball",
    "task_id": "HumanEval/109",
    "Failed": "Timeout of 60.0 s. has been hit during disagreement computation"
  },
  {
    "name": "exchange",
    "task_id": "HumanEval/110",
    "Dis": 0.311,
    "Err": 0.2613333333333333,
    "TotalTime": 5.006857395172119
  },
  {
    "name": "histogram",
    "task_id": "HumanEval/111",
    "Dis": 0.005166666666666667,
    "Err": 0.31366666666666665,
    "TotalTime": 2.2717649936676025
  },
  {
    "name": "reverse_delete",
    "task_id": "HumanEval/112",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.883380174636841
  },
  {
    "name": "odd_count",
    "task_id": "HumanEval/113",
    "Dis": 0.7601666666666667,
    "Err": 0.5861666666666666,
    "TotalTime": 5.2255377769470215
  },
  {
    "name": "minSubArraySum",
    "task_id": "HumanEval/114",
    "Dis": 0.40016666666666667,
    "Err": 0.26066666666666666,
    "TotalTime": 2.9480957984924316
  },
  {
    "name": "max_fill",
    "task_id": "HumanEval/115",
    "Dis": 0.6245,
    "Err": 0.602,
    "TotalTime": 4.760666608810425
  },
  {
    "name": "sort_array",
    "task_id": "HumanEval/116",
    "Dis": 0.08,
    "Err": 0.03783333333333333,
    "TotalTime": 3.1895852088928223
  },
  {
    "name": "select_words",
    "task_id": "HumanEval/117",
    "Dis": 0.058333333333333334,
    "Err": 0.04516666666666667,
    "TotalTime": 2.49444842338562
  },
  {
    "name": "get_closest_vowel",
    "task_id": "HumanEval/118",
    "Dis": 0.44433333333333336,
    "Err": 0.3451666666666667,
    "TotalTime": 2.217952013015747
  },
  {
    "name": "match_parens",
    "task_id": "HumanEval/119",
    "Dis": 0.4835,
    "Err": 0.4285,
    "TotalTime": 3.691128969192505
  },
  {
    "name": "maximum",
    "task_id": "HumanEval/120",
    "Dis": 0.10833333333333334,
    "Err": 0.5573333333333333,
    "TotalTime": 3.120425224304199
  },
  {
    "name": "solution",
    "task_id": "HumanEval/121",
    "Dis": 0.0,
    "Err": 0.027333333333333334,
    "TotalTime": 2.6313819885253906
  },
  {
    "name": "add_elements",
    "task_id": "HumanEval/122",
    "Dis": 0.46,
    "Err": 0.36433333333333334,
    "TotalTime": 2.90664005279541
  },
  {
    "name": "get_odd_collatz",
    "task_id": "HumanEval/123",
    "Failed": "Timeout of 60.0 s. has been hit during disagreement computation"
  },
  {
    "name": "valid_date",
    "task_id": "HumanEval/124",
    "Dis": 0.06283333333333334,
    "Err": 0.041833333333333333,
    "TotalTime": 2.26070499420166
  },
  {
    "name": "split_words",
    "task_id": "HumanEval/125",
    "Dis": 0.6156666666666667,
    "Err": 0.4975,
    "TotalTime": 2.23406982421875
  },
  {
    "name": "is_sorted",
    "task_id": "HumanEval/126",
    "Dis": 0.07683333333333334,
    "Err": 0.09033333333333333,
    "TotalTime": 2.9490413665771484
  },
  {
    "name": "intersection",
    "task_id": "HumanEval/127",
    "Dis": 0.2545,
    "Err": 0.6351666666666667,
    "TotalTime": 6.818659543991089
  },
  {
    "name": "prod_signs",
    "task_id": "HumanEval/128",
    "Dis": 0.22333333333333333,
    "Err": 0.31383333333333335,
    "TotalTime": 3.0305733680725098
  },
  {
    "name": "minPath",
    "task_id": "HumanEval/129",
    "Dis": 0.884,
    "Err": 0.8928333333333334,
    "TotalTime": 16.15019679069519
  },
  {
    "name": "tri",
    "task_id": "HumanEval/130",
    "Dis": 0.8638333333333333,
    "Err": 0.983,
    "TotalTime": 7.983839988708496
  },
  {
    "name": "digits",
    "task_id": "HumanEval/131",
    "Dis": 0.20483333333333334,
    "Err": 0.16,
    "TotalTime": 1.7824749946594238
  },
  {
    "name": "is_nested",
    "task_id": "HumanEval/132",
    "Dis": 0.21783333333333332,
    "Err": 0.25533333333333336,
    "TotalTime": 2.238692045211792
  },
  {
    "name": "sum_squares",
    "task_id": "HumanEval/133",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.00488018989563
  },
  {
    "name": "check_if_last_char_is_a_letter",
    "task_id": "HumanEval/134",
    "Dis": 0.2713333333333333,
    "Err": 0.35183333333333333,
    "TotalTime": 2.255530834197998
  },
  {
    "name": "can_arrange",
    "task_id": "HumanEval/135",
    "Dis": 0.49,
    "Err": 0.5935,
    "TotalTime": 3.0413873195648193
  },
  {
    "name": "largest_smallest_integers",
    "task_id": "HumanEval/136",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.1847305297851562
  },
  {
    "name": "compare_one",
    "task_id": "HumanEval/137",
    "Dis": 0.4671666666666667,
    "Err": 0.2846666666666667,
    "TotalTime": 3.098360061645508
  },
  {
    "name": "is_equal_to_sum_even",
    "task_id": "HumanEval/138",
    "Dis": 0.13883333333333334,
    "Err": 0.10383333333333333,
    "TotalTime": 15.863932132720947
  },
  {
    "name": "special_factorial",
    "task_id": "HumanEval/139",
    "Failed": "Timeout of 60.0 s. has been hit during disagreement computation"
  },
  {
    "name": "fix_spaces",
    "task_id": "HumanEval/140",
    "Dis": 0.4573333333333333,
    "Err": 0.3845,
    "TotalTime": 2.3834598064422607
  },
  {
    "name": "file_name_check",
    "task_id": "HumanEval/141",
    "Dis": 0.012833333333333334,
    "Err": 0.006166666666666667,
    "TotalTime": 2.3110997676849365
  },
  {
    "name": "sum_squares",
    "task_id": "HumanEval/142",
    "Dis": 0.615,
    "Err": 0.502,
    "TotalTime": 3.2495779991149902
  },
  {
    "name": "words_in_sentence",
    "task_id": "HumanEval/143",
    "Dis": 0.10366666666666667,
    "Err": 0.048,
    "TotalTime": 2.4779772758483887
  },
  {
    "name": "simplify",
    "task_id": "HumanEval/144",
    "Dis": 0.0006666666666666666,
    "Err": 0.0015,
    "TotalTime": 86.09366607666016
  },
  {
    "name": "order_by_points",
    "task_id": "HumanEval/145",
    "Dis": 0.7586666666666667,
    "Err": 0.9093333333333333,
    "TotalTime": 3.8663065433502197
  },
  {
    "name": "specialFilter",
    "task_id": "HumanEval/146",
    "Dis": 0.1505,
    "Err": 0.07316666666666667,
    "TotalTime": 3.1571013927459717
  },
  {
    "name": "bf",
    "task_id": "HumanEval/148",
    "Dis": 0.20333333333333334,
    "Err": 0.09383333333333334,
    "TotalTime": 2.9904160499572754
  },
  {
    "name": "sorted_list_sum",
    "task_id": "HumanEval/149",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.949392795562744
  },
  {
    "name": "x_or_y",
    "task_id": "HumanEval/150",
    "Dis": 0.0,
    "Err": 0.22066666666666668,
    "TotalTime": 2.6606690883636475
  },
  {
    "name": "double_the_difference",
    "task_id": "HumanEval/151",
    "Dis": 0.28983333333333333,
    "Err": 0.21883333333333332,
    "TotalTime": 4.80784797668457
  },
  {
    "name": "compare",
    "task_id": "HumanEval/152",
    "Dis": 0.45166666666666666,
    "Err": 0.4711666666666667,
    "TotalTime": 5.441352128982544
  },
  {
    "name": "Strongest_Extension",
    "task_id": "HumanEval/153",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.985405683517456
  },
  {
    "name": "cycpattern_check",
    "task_id": "HumanEval/154",
    "Dis": 0.027833333333333335,
    "Err": 0.025166666666666667,
    "TotalTime": 3.045593500137329
  },
  {
    "name": "even_odd_count",
    "task_id": "HumanEval/155",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.7904820442199707
  },
  {
    "name": "right_angle_triangle",
    "task_id": "HumanEval/157",
    "Dis": 0.12833333333333333,
    "Err": 0.06916666666666667,
    "TotalTime": 2.2979214191436768
  },
  {
    "name": "find_max",
    "task_id": "HumanEval/158",
    "Dis": 0.058666666666666666,
    "Err": 0.05616666666666666,
    "TotalTime": 3.6660215854644775
  },
  {
    "name": "eat",
    "task_id": "HumanEval/159",
    "Dis": 0.712,
    "Err": 0.7278333333333333,
    "TotalTime": 2.2338247299194336
  },
  {
    "name": "solve",
    "task_id": "HumanEval/161",
    "Dis": 0.3436666666666667,
    "Err": 0.21883333333333332,
    "TotalTime": 2.3464677333831787
  },
  {
    "name": "string_to_md5",
    "task_id": "HumanEval/162",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.71437668800354
  },
  {
    "name": "generate_integers",
    "task_id": "HumanEval/163",
    "Dis": 0.214,
    "Err": 0.8453333333333334,
    "TotalTime": 2.081941604614258
  }
]