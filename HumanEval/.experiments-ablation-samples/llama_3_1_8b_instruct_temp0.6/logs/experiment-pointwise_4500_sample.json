[
  {
    "name": "has_close_elements",
    "task_id": "HumanEval/0",
    "Dis": 0.0,
    "Err": 0.059555555555555556,
    "TotalTime": 2.6825547218322754
  },
  {
    "name": "separate_paren_groups",
    "task_id": "HumanEval/1",
    "Dis": 0.7553333333333333,
    "Err": 0.6475555555555556,
    "TotalTime": 1.9554815292358398
  },
  {
    "name": "truncate_number",
    "task_id": "HumanEval/2",
    "Dis": 0.078,
    "Err": 0.17155555555555554,
    "TotalTime": 1.570406436920166
  },
  {
    "name": "below_zero",
    "task_id": "HumanEval/3",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.3783721923828125
  },
  {
    "name": "mean_absolute_deviation",
    "task_id": "HumanEval/4",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.6699059009552
  },
  {
    "name": "intersperse",
    "task_id": "HumanEval/5",
    "Dis": 0.04644444444444444,
    "Err": 0.019555555555555555,
    "TotalTime": 2.3049795627593994
  },
  {
    "name": "parse_nested_parens",
    "task_id": "HumanEval/6",
    "Dis": 0.18644444444444444,
    "Err": 0.4335555555555556,
    "TotalTime": 1.980330228805542
  },
  {
    "name": "filter_by_substring",
    "task_id": "HumanEval/7",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.3913421630859375
  },
  {
    "name": "sum_product",
    "task_id": "HumanEval/8",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.509291887283325
  },
  {
    "name": "rolling_max",
    "task_id": "HumanEval/9",
    "Dis": 0.04755555555555555,
    "Err": 0.0811111111111111,
    "TotalTime": 2.4152133464813232
  },
  {
    "name": "make_palindrome",
    "task_id": "HumanEval/10",
    "Dis": 0.7597777777777778,
    "Err": 0.6413333333333333,
    "TotalTime": 1.992539405822754
  },
  {
    "name": "string_xor",
    "task_id": "HumanEval/11",
    "Dis": 0.7333333333333333,
    "Err": 0.7446666666666667,
    "TotalTime": 2.3991599082946777
  },
  {
    "name": "longest",
    "task_id": "HumanEval/12",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.722395658493042
  },
  {
    "name": "greatest_common_divisor",
    "task_id": "HumanEval/13",
    "Dis": 0.11644444444444445,
    "Err": 0.11066666666666666,
    "TotalTime": 1.6682088375091553
  },
  {
    "name": "all_prefixes",
    "task_id": "HumanEval/14",
    "Dis": 0.14577777777777778,
    "Err": 0.07377777777777778,
    "TotalTime": 1.9529602527618408
  },
  {
    "name": "string_sequence",
    "task_id": "HumanEval/15",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.583324670791626
  },
  {
    "name": "count_distinct_characters",
    "task_id": "HumanEval/16",
    "Dis": 0.256,
    "Err": 0.14422222222222222,
    "TotalTime": 2.0509109497070312
  },
  {
    "name": "parse_music",
    "task_id": "HumanEval/17",
    "Dis": 0.3217777777777778,
    "Err": 0.17222222222222222,
    "TotalTime": 4.091116428375244
  },
  {
    "name": "how_many_times",
    "task_id": "HumanEval/18",
    "Dis": 0.1608888888888889,
    "Err": 0.1068888888888889,
    "TotalTime": 2.498945713043213
  },
  {
    "name": "sort_numbers",
    "task_id": "HumanEval/19",
    "Dis": 0.11866666666666667,
    "Err": 0.064,
    "TotalTime": 5.947612047195435
  },
  {
    "name": "find_closest_elements",
    "task_id": "HumanEval/20",
    "Dis": 0.03422222222222222,
    "Err": 0.03222222222222222,
    "TotalTime": 2.7365095615386963
  },
  {
    "name": "rescale_to_unit",
    "task_id": "HumanEval/21",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.737100601196289
  },
  {
    "name": "filter_integers",
    "task_id": "HumanEval/22",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.5337483882904053
  },
  {
    "name": "strlen",
    "task_id": "HumanEval/23",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.8734064102172852
  },
  {
    "name": "largest_divisor",
    "task_id": "HumanEval/24",
    "Dis": 0.08377777777777778,
    "Err": 0.19266666666666668,
    "TotalTime": 1.5785343647003174
  },
  {
    "name": "factorize",
    "task_id": "HumanEval/25",
    "Dis": 0.136,
    "Err": 0.07577777777777778,
    "TotalTime": 5.430238246917725
  },
  {
    "name": "remove_duplicates",
    "task_id": "HumanEval/26",
    "Dis": 0.0,
    "Err": 0.4562222222222222,
    "TotalTime": 2.5514116287231445
  },
  {
    "name": "flip_case",
    "task_id": "HumanEval/27",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.9809250831604004
  },
  {
    "name": "concatenate",
    "task_id": "HumanEval/28",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.18981671333313
  },
  {
    "name": "filter_by_prefix",
    "task_id": "HumanEval/29",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.274505853652954
  },
  {
    "name": "get_positive",
    "task_id": "HumanEval/30",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.720163583755493
  },
  {
    "name": "is_prime",
    "task_id": "HumanEval/31",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.531947612762451
  },
  {
    "name": "unique",
    "task_id": "HumanEval/34",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.7168989181518555
  },
  {
    "name": "max_element",
    "task_id": "HumanEval/35",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.7400639057159424
  },
  {
    "name": "sort_even",
    "task_id": "HumanEval/37",
    "Dis": 0.3635555555555556,
    "Err": 0.18155555555555555,
    "TotalTime": 2.6711361408233643
  },
  {
    "name": "triples_sum_to_zero",
    "task_id": "HumanEval/40",
    "Dis": 0.10644444444444444,
    "Err": 0.07133333333333333,
    "TotalTime": 2.572277069091797
  },
  {
    "name": "car_race_collision",
    "task_id": "HumanEval/41",
    "Dis": 0.6928888888888889,
    "Err": 0.9704444444444444,
    "TotalTime": 1.6547105312347412
  },
  {
    "name": "incr_list",
    "task_id": "HumanEval/42",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.6928412914276123
  },
  {
    "name": "pairs_sum_to_zero",
    "task_id": "HumanEval/43",
    "Dis": 0.0,
    "Err": 0.006888888888888889,
    "TotalTime": 2.566432237625122
  },
  {
    "name": "triangle_area",
    "task_id": "HumanEval/45",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.6779873371124268
  },
  {
    "name": "fib4",
    "task_id": "HumanEval/46",
    "Failed": "Exception occurred during compilation of LLM-generated code -- TimeoutError : "
  },
  {
    "name": "median",
    "task_id": "HumanEval/47",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.633704423904419
  },
  {
    "name": "is_palindrome",
    "task_id": "HumanEval/48",
    "Dis": 0.003111111111111111,
    "Err": 0.0006666666666666666,
    "TotalTime": 2.0089707374572754
  },
  {
    "name": "modp",
    "task_id": "HumanEval/49",
    "Dis": 0.07377777777777778,
    "Err": 0.208,
    "TotalTime": 2.9370906352996826
  },
  {
    "name": "remove_vowels",
    "task_id": "HumanEval/51",
    "Dis": 0.214,
    "Err": 0.11733333333333333,
    "TotalTime": 1.9867818355560303
  },
  {
    "name": "below_threshold",
    "task_id": "HumanEval/52",
    "Dis": 0.004222222222222222,
    "Err": 0.0026666666666666666,
    "TotalTime": 2.602297067642212
  },
  {
    "name": "add",
    "task_id": "HumanEval/53",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.734621524810791
  },
  {
    "name": "same_chars",
    "task_id": "HumanEval/54",
    "Dis": 0.003777777777777778,
    "Err": 0.0035555555555555557,
    "TotalTime": 2.503969669342041
  },
  {
    "name": "correct_bracketing",
    "task_id": "HumanEval/56",
    "Dis": 0.0,
    "Err": 0.4002222222222222,
    "TotalTime": 2.0036063194274902
  },
  {
    "name": "monotonic",
    "task_id": "HumanEval/57",
    "Dis": 0.022444444444444444,
    "Err": 0.012444444444444444,
    "TotalTime": 2.544550657272339
  },
  {
    "name": "common",
    "task_id": "HumanEval/58",
    "Dis": 0.22177777777777777,
    "Err": 0.5006666666666667,
    "TotalTime": 3.0044362545013428
  },
  {
    "name": "largest_prime_factor",
    "task_id": "HumanEval/59",
    "Dis": 0.082,
    "Err": 0.07622222222222222,
    "TotalTime": 4.949595212936401
  },
  {
    "name": "sum_to_n",
    "task_id": "HumanEval/60",
    "Dis": 0.10822222222222222,
    "Err": 0.16955555555555554,
    "TotalTime": 1.542635440826416
  },
  {
    "name": "correct_bracketing",
    "task_id": "HumanEval/61",
    "Dis": 0.035555555555555556,
    "Err": 0.42644444444444446,
    "TotalTime": 1.9517796039581299
  },
  {
    "name": "derivative",
    "task_id": "HumanEval/62",
    "Dis": 0.5335555555555556,
    "Err": 0.29088888888888886,
    "TotalTime": 2.4078145027160645
  },
  {
    "name": "vowels_count",
    "task_id": "HumanEval/64",
    "Dis": 0.04088888888888889,
    "Err": 0.022444444444444444,
    "TotalTime": 1.957948923110962
  },
  {
    "name": "circular_shift",
    "task_id": "HumanEval/65",
    "Dis": 0.29577777777777775,
    "Err": 0.388,
    "TotalTime": 1.7037959098815918
  },
  {
    "name": "digitSum",
    "task_id": "HumanEval/66",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.9807257652282715
  },
  {
    "name": "fruit_distribution",
    "task_id": "HumanEval/67",
    "Dis": 0.5528888888888889,
    "Err": 0.7955555555555556,
    "TotalTime": 2.3085577487945557
  },
  {
    "name": "pluck",
    "task_id": "HumanEval/68",
    "Dis": 0.1902222222222222,
    "Err": 0.084,
    "TotalTime": 2.6580705642700195
  },
  {
    "name": "search",
    "task_id": "HumanEval/69",
    "Dis": 0.11088888888888888,
    "Err": 0.16822222222222222,
    "TotalTime": 3.1747052669525146
  },
  {
    "name": "strange_sort_list",
    "task_id": "HumanEval/70",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.444479465484619
  },
  {
    "name": "triangle_area",
    "task_id": "HumanEval/71",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.9412295818328857
  },
  {
    "name": "will_it_fly",
    "task_id": "HumanEval/72",
    "Dis": 0.010444444444444444,
    "Err": 0.006,
    "TotalTime": 2.7627577781677246
  },
  {
    "name": "smallest_change",
    "task_id": "HumanEval/73",
    "Dis": 0.6206666666666667,
    "Err": 0.7733333333333333,
    "TotalTime": 2.2938709259033203
  },
  {
    "name": "total_match",
    "task_id": "HumanEval/74",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 5.298416376113892
  },
  {
    "name": "iscube",
    "task_id": "HumanEval/77",
    "Dis": 0.4022222222222222,
    "Err": 0.38422222222222224,
    "TotalTime": 1.4923772811889648
  },
  {
    "name": "hex_key",
    "task_id": "HumanEval/78",
    "Dis": 0.21822222222222223,
    "Err": 0.1731111111111111,
    "TotalTime": 2.1024746894836426
  },
  {
    "name": "decimal_to_binary",
    "task_id": "HumanEval/79",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.549328327178955
  },
  {
    "name": "is_happy",
    "task_id": "HumanEval/80",
    "Dis": 0.164,
    "Err": 0.0748888888888889,
    "TotalTime": 1.9285852909088135
  },
  {
    "name": "numerical_letter_grade",
    "task_id": "HumanEval/81",
    "Dis": 0.5286666666666666,
    "Err": 0.49,
    "TotalTime": 2.6403675079345703
  },
  {
    "name": "prime_length",
    "task_id": "HumanEval/82",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.0087502002716064
  },
  {
    "name": "starts_one_ends",
    "task_id": "HumanEval/83",
    "Dis": 0.8126666666666666,
    "Err": 0.9857777777777778,
    "TotalTime": 1.6054644584655762
  },
  {
    "name": "solve",
    "task_id": "HumanEval/84",
    "Dis": 0.5251111111111111,
    "Err": 0.8975555555555556,
    "TotalTime": 1.6343755722045898
  },
  {
    "name": "add",
    "task_id": "HumanEval/85",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.421468734741211
  },
  {
    "name": "anti_shuffle",
    "task_id": "HumanEval/86",
    "Dis": 0.3108888888888889,
    "Err": 0.3877777777777778,
    "TotalTime": 1.9862539768218994
  },
  {
    "name": "get_row",
    "task_id": "HumanEval/87",
    "Dis": 0.12,
    "Err": 0.09822222222222222,
    "TotalTime": 4.385991096496582
  },
  {
    "name": "sort_array",
    "task_id": "HumanEval/88",
    "Dis": 0.1622222222222222,
    "Err": 0.08711111111111111,
    "TotalTime": 2.6925621032714844
  },
  {
    "name": "encrypt",
    "task_id": "HumanEval/89",
    "Dis": 0.4597777777777778,
    "Err": 0.44755555555555554,
    "TotalTime": 1.9793217182159424
  },
  {
    "name": "next_smallest",
    "task_id": "HumanEval/90",
    "Dis": 0.244,
    "Err": 0.16977777777777778,
    "TotalTime": 2.5960452556610107
  },
  {
    "name": "is_bored",
    "task_id": "HumanEval/91",
    "Dis": 0.03333333333333333,
    "Err": 0.15133333333333332,
    "TotalTime": 2.039152145385742
  },
  {
    "name": "any_int",
    "task_id": "HumanEval/92",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.018519878387451
  },
  {
    "name": "encode",
    "task_id": "HumanEval/93",
    "Dis": 0.6186666666666667,
    "Err": 0.6704444444444444,
    "TotalTime": 2.044731855392456
  },
  {
    "name": "skjkasdkd",
    "task_id": "HumanEval/94",
    "Dis": 0.06866666666666667,
    "Err": 0.07377777777777778,
    "TotalTime": 3.321218490600586
  },
  {
    "name": "check_dict_case",
    "task_id": "HumanEval/95",
    "Dis": 0.1688888888888889,
    "Err": 0.10977777777777778,
    "TotalTime": 2.6054069995880127
  },
  {
    "name": "count_up_to",
    "task_id": "HumanEval/96",
    "Dis": 0.04488888888888889,
    "Err": 0.028,
    "TotalTime": 2.034191608428955
  },
  {
    "name": "multiply",
    "task_id": "HumanEval/97",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.7686731815338135
  },
  {
    "name": "count_upper",
    "task_id": "HumanEval/98",
    "Dis": 0.10466666666666667,
    "Err": 0.0791111111111111,
    "TotalTime": 1.935617446899414
  },
  {
    "name": "closest_integer",
    "task_id": "HumanEval/99",
    "Dis": 0.37622222222222224,
    "Err": 0.23755555555555555,
    "TotalTime": 4.02520227432251
  },
  {
    "name": "make_a_pile",
    "task_id": "HumanEval/100",
    "Failed": "Timeout of 60.0 s. has been hit during disagreement computation"
  },
  {
    "name": "words_string",
    "task_id": "HumanEval/101",
    "Dis": 0.04511111111111111,
    "Err": 0.025333333333333333,
    "TotalTime": 2.007957696914673
  },
  {
    "name": "choose_num",
    "task_id": "HumanEval/102",
    "Dis": 0.3433333333333333,
    "Err": 0.42644444444444446,
    "TotalTime": 2.287513494491577
  },
  {
    "name": "rounded_avg",
    "task_id": "HumanEval/103",
    "Dis": 0.4671111111111111,
    "Err": 0.4613333333333333,
    "TotalTime": 1.9023463726043701
  },
  {
    "name": "unique_digits",
    "task_id": "HumanEval/104",
    "Dis": 0.0015555555555555555,
    "Err": 0.002,
    "TotalTime": 2.93710994720459
  },
  {
    "name": "by_length",
    "task_id": "HumanEval/105",
    "Dis": 0.36822222222222223,
    "Err": 0.2757777777777778,
    "TotalTime": 2.5669798851013184
  },
  {
    "name": "f",
    "task_id": "HumanEval/106",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.8515591621398926
  },
  {
    "name": "even_odd_palindrome",
    "task_id": "HumanEval/107",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.9430320262908936
  },
  {
    "name": "count_nums",
    "task_id": "HumanEval/108",
    "Dis": 0.46155555555555555,
    "Err": 0.49733333333333335,
    "TotalTime": 2.837547779083252
  },
  {
    "name": "move_one_ball",
    "task_id": "HumanEval/109",
    "Failed": "Timeout of 60.0 s. has been hit during disagreement computation"
  },
  {
    "name": "exchange",
    "task_id": "HumanEval/110",
    "Dis": 0.30933333333333335,
    "Err": 0.25266666666666665,
    "TotalTime": 4.248403072357178
  },
  {
    "name": "histogram",
    "task_id": "HumanEval/111",
    "Dis": 0.003777777777777778,
    "Err": 0.3244444444444444,
    "TotalTime": 2.012667417526245
  },
  {
    "name": "reverse_delete",
    "task_id": "HumanEval/112",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.3731696605682373
  },
  {
    "name": "odd_count",
    "task_id": "HumanEval/113",
    "Dis": 0.7686666666666667,
    "Err": 0.5875555555555556,
    "TotalTime": 4.178833484649658
  },
  {
    "name": "minSubArraySum",
    "task_id": "HumanEval/114",
    "Dis": 0.3897777777777778,
    "Err": 0.2571111111111111,
    "TotalTime": 2.4451980590820312
  },
  {
    "name": "max_fill",
    "task_id": "HumanEval/115",
    "Dis": 0.6193333333333333,
    "Err": 0.5931111111111111,
    "TotalTime": 3.8416876792907715
  },
  {
    "name": "sort_array",
    "task_id": "HumanEval/116",
    "Dis": 0.07466666666666667,
    "Err": 0.03822222222222222,
    "TotalTime": 2.561927556991577
  },
  {
    "name": "select_words",
    "task_id": "HumanEval/117",
    "Dis": 0.06066666666666667,
    "Err": 0.044444444444444446,
    "TotalTime": 2.267101526260376
  },
  {
    "name": "get_closest_vowel",
    "task_id": "HumanEval/118",
    "Dis": 0.45066666666666666,
    "Err": 0.3406666666666667,
    "TotalTime": 1.8563728332519531
  },
  {
    "name": "match_parens",
    "task_id": "HumanEval/119",
    "Dis": 0.48577777777777775,
    "Err": 0.4315555555555556,
    "TotalTime": 2.999420642852783
  },
  {
    "name": "maximum",
    "task_id": "HumanEval/120",
    "Dis": 0.10777777777777778,
    "Err": 0.5486666666666666,
    "TotalTime": 2.678187370300293
  },
  {
    "name": "solution",
    "task_id": "HumanEval/121",
    "Dis": 0.0,
    "Err": 0.024,
    "TotalTime": 2.345470666885376
  },
  {
    "name": "add_elements",
    "task_id": "HumanEval/122",
    "Dis": 0.4602222222222222,
    "Err": 0.3655555555555556,
    "TotalTime": 2.566983699798584
  },
  {
    "name": "get_odd_collatz",
    "task_id": "HumanEval/123",
    "Failed": "Timeout of 60.0 s. has been hit during disagreement computation"
  },
  {
    "name": "valid_date",
    "task_id": "HumanEval/124",
    "Dis": 0.06911111111111111,
    "Err": 0.050222222222222224,
    "TotalTime": 1.9767746925354004
  },
  {
    "name": "split_words",
    "task_id": "HumanEval/125",
    "Dis": 0.6195555555555555,
    "Err": 0.49377777777777776,
    "TotalTime": 1.9283289909362793
  },
  {
    "name": "is_sorted",
    "task_id": "HumanEval/126",
    "Dis": 0.07666666666666666,
    "Err": 0.08177777777777778,
    "TotalTime": 2.4958019256591797
  },
  {
    "name": "intersection",
    "task_id": "HumanEval/127",
    "Dis": 0.25133333333333335,
    "Err": 0.6484444444444445,
    "TotalTime": 5.22263503074646
  },
  {
    "name": "prod_signs",
    "task_id": "HumanEval/128",
    "Dis": 0.21977777777777777,
    "Err": 0.3091111111111111,
    "TotalTime": 2.548447370529175
  },
  {
    "name": "minPath",
    "task_id": "HumanEval/129",
    "Dis": 0.8784444444444445,
    "Err": 0.8944444444444445,
    "TotalTime": 11.79190731048584
  },
  {
    "name": "tri",
    "task_id": "HumanEval/130",
    "Dis": 0.8831111111111111,
    "Err": 0.9784444444444444,
    "TotalTime": 6.048717737197876
  },
  {
    "name": "digits",
    "task_id": "HumanEval/131",
    "Dis": 0.19622222222222221,
    "Err": 0.16066666666666668,
    "TotalTime": 1.612680435180664
  },
  {
    "name": "is_nested",
    "task_id": "HumanEval/132",
    "Dis": 0.21911111111111112,
    "Err": 0.26066666666666666,
    "TotalTime": 2.0659918785095215
  },
  {
    "name": "sum_squares",
    "task_id": "HumanEval/133",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.586808919906616
  },
  {
    "name": "check_if_last_char_is_a_letter",
    "task_id": "HumanEval/134",
    "Dis": 0.2777777777777778,
    "Err": 0.3595555555555556,
    "TotalTime": 1.9104595184326172
  },
  {
    "name": "can_arrange",
    "task_id": "HumanEval/135",
    "Dis": 0.48844444444444446,
    "Err": 0.5928888888888889,
    "TotalTime": 2.5052080154418945
  },
  {
    "name": "largest_smallest_integers",
    "task_id": "HumanEval/136",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.6620357036590576
  },
  {
    "name": "compare_one",
    "task_id": "HumanEval/137",
    "Dis": 0.4588888888888889,
    "Err": 0.27666666666666667,
    "TotalTime": 2.609752655029297
  },
  {
    "name": "is_equal_to_sum_even",
    "task_id": "HumanEval/138",
    "Dis": 0.134,
    "Err": 0.11244444444444444,
    "TotalTime": 14.561086177825928
  },
  {
    "name": "special_factorial",
    "task_id": "HumanEval/139",
    "Failed": "Timeout of 60.0 s. has been hit during disagreement computation"
  },
  {
    "name": "fix_spaces",
    "task_id": "HumanEval/140",
    "Dis": 0.45355555555555555,
    "Err": 0.3962222222222222,
    "TotalTime": 1.982630968093872
  },
  {
    "name": "file_name_check",
    "task_id": "HumanEval/141",
    "Dis": 0.012888888888888889,
    "Err": 0.007333333333333333,
    "TotalTime": 1.9050638675689697
  },
  {
    "name": "sum_squares",
    "task_id": "HumanEval/142",
    "Dis": 0.6084444444444445,
    "Err": 0.496,
    "TotalTime": 2.7416608333587646
  },
  {
    "name": "words_in_sentence",
    "task_id": "HumanEval/143",
    "Dis": 0.09911111111111111,
    "Err": 0.052,
    "TotalTime": 2.1062073707580566
  },
  {
    "name": "simplify",
    "task_id": "HumanEval/144",
    "Dis": 0.002,
    "Err": 0.0011111111111111111,
    "TotalTime": 64.11782050132751
  },
  {
    "name": "order_by_points",
    "task_id": "HumanEval/145",
    "Dis": 0.758,
    "Err": 0.9186666666666666,
    "TotalTime": 2.9279603958129883
  },
  {
    "name": "specialFilter",
    "task_id": "HumanEval/146",
    "Dis": 0.15866666666666668,
    "Err": 0.076,
    "TotalTime": 2.5935161113739014
  },
  {
    "name": "bf",
    "task_id": "HumanEval/148",
    "Dis": 0.20222222222222222,
    "Err": 0.098,
    "TotalTime": 2.609071969985962
  },
  {
    "name": "sorted_list_sum",
    "task_id": "HumanEval/149",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 3.292527198791504
  },
  {
    "name": "x_or_y",
    "task_id": "HumanEval/150",
    "Dis": 0.0,
    "Err": 0.21777777777777776,
    "TotalTime": 2.368497133255005
  },
  {
    "name": "double_the_difference",
    "task_id": "HumanEval/151",
    "Dis": 0.2862222222222222,
    "Err": 0.21533333333333332,
    "TotalTime": 3.874236822128296
  },
  {
    "name": "compare",
    "task_id": "HumanEval/152",
    "Dis": 0.4553333333333333,
    "Err": 0.4666666666666667,
    "TotalTime": 4.252424955368042
  },
  {
    "name": "Strongest_Extension",
    "task_id": "HumanEval/153",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 4.031088829040527
  },
  {
    "name": "cycpattern_check",
    "task_id": "HumanEval/154",
    "Dis": 0.024,
    "Err": 0.030222222222222223,
    "TotalTime": 2.5569074153900146
  },
  {
    "name": "even_odd_count",
    "task_id": "HumanEval/155",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 1.6245999336242676
  },
  {
    "name": "right_angle_triangle",
    "task_id": "HumanEval/157",
    "Dis": 0.12844444444444444,
    "Err": 0.0651111111111111,
    "TotalTime": 1.9366755485534668
  },
  {
    "name": "find_max",
    "task_id": "HumanEval/158",
    "Dis": 0.05377777777777778,
    "Err": 0.05977777777777778,
    "TotalTime": 3.0803911685943604
  },
  {
    "name": "eat",
    "task_id": "HumanEval/159",
    "Dis": 0.7086666666666667,
    "Err": 0.7462222222222222,
    "TotalTime": 1.9260406494140625
  },
  {
    "name": "solve",
    "task_id": "HumanEval/161",
    "Dis": 0.35555555555555557,
    "Err": 0.22022222222222224,
    "TotalTime": 1.9269418716430664
  },
  {
    "name": "string_to_md5",
    "task_id": "HumanEval/162",
    "Dis": 0.0,
    "Err": 0.0,
    "TotalTime": 2.228926658630371
  },
  {
    "name": "generate_integers",
    "task_id": "HumanEval/163",
    "Dis": 0.21288888888888888,
    "Err": 0.8533333333333334,
    "TotalTime": 1.792353630065918
  }
]